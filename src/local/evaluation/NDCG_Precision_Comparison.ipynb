{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help to compare NDCG and Precision scores for the BERT re-ranking results on the MS MARCO dataset making use of the newly acquired relevance label dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm.auto import tqdm \n",
    "from tqdm import tqdm_notebook\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "\n",
    "# binary threshold (irrelevant <2; relevant >= 2)\n",
    "thesis_qrels_threshold2_filename = 'thesis_dataset_binary_threshold2.tsv'\n",
    "\n",
    "# binary theshold (irrelevant <3; relevant >= 3)\n",
    "thesis_qrels_threshold3_filename = 'thesis_dataset_binary_threshold3.tsv'\n",
    "\n",
    "# graded relevance file\n",
    "new_qrels_filename = 'thesis_dataset_graded_relevance.tsv'\n",
    "\n",
    "# msmarco relevance file\n",
    "og_qrels_filename = 'qrels.dev.small.tsv'\n",
    "\n",
    "# BM25 top 100 ranking\n",
    "bm25_top100_filename = 'run_development_top100.tsv'\n",
    "\n",
    "# BERT top 100 ranking\n",
    "bert_top100_filename = 'bert_thesis_dataset_top100.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_df = pd.read_csv(data_dir + bm25_top100_filename,delimiter='\\t',encoding='utf-8',header=None)\n",
    "bm25_df.columns = ['query_id', 'passage_id', 'bm25_rank']\n",
    "\n",
    "bert_df = pd.read_csv(data_dir + bert_top100_filename,delimiter='\\t',encoding='utf-8',header=None)\n",
    "bert_df.columns = ['query_id', 'passage_id', 'bm25_rank', 'query', 'passage', 'bert_score', 'bert_rank']\n",
    "\n",
    "og_qrels_df = pd.read_csv(data_dir + og_qrels_filename,delimiter='\\t',encoding='utf-8',header=None)\n",
    "og_qrels_df.columns = ['query_id','label1','passage_id','label2']\n",
    "\n",
    "new_graded_qrels_df = pd.read_csv(data_dir + new_qrels_filename,delimiter='\\t',encoding='utf-8',header=None)\n",
    "new_graded_qrels_df.columns = ['query_id','passage_id','graded_label']\n",
    "\n",
    "new_qrels2_df = pd.read_csv(data_dir + thesis_qrels_threshold2_filename,delimiter='\\t',encoding='utf-8',header=None)\n",
    "new_qrels2_df.columns = ['query_id','label1','passage_id','label2']\n",
    "\n",
    "new_qrels3_df = pd.read_csv(data_dir + thesis_qrels_threshold3_filename,delimiter='\\t',encoding='utf-8',header=None)\n",
    "new_qrels3_df.columns = ['query_id','label1','passage_id','label2']\n",
    "\n",
    "models_dict = {\"bm25\": bm25_df, \"bert\": bert_df}\n",
    "new_qrels_dict = {\"graded\": new_graded_qrels_df, \"binary2\": new_qrels2_df, \"binary3\": new_qrels3_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_ids(dataframe):\n",
    "    return list(np.unique(dataframe['query_id'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_ids(dataframe):\n",
    "    relevant_passages = dataframe['passage_id'].values.tolist()\n",
    "    return relevant_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_ranking(dataframe,rank_column,n):\n",
    "    top_n_ranking = dataframe[dataframe[rank_column] <= n].sort_values(by=[rank_column])\n",
    "    return top_n_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discounted_gain(index,rel):\n",
    "    dg = rel/math.log((index+2),2)\n",
    "    return dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discounted_cumulative_gain(relevance_labels):\n",
    "    dcg = 0.0\n",
    "    for index, rel in enumerate(relevance_labels):\n",
    "        dcg += compute_discounted_gain(index,rel)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg(relevance_labels):\n",
    "    dcg = compute_discounted_cumulative_gain(relevance_labels)\n",
    "    idcg = compute_discounted_cumulative_gain(sorted(relevance_labels,reverse=True))\n",
    "    ndcg = dcg/idcg\n",
    "    return round(ndcg,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(gt,ranking,n):\n",
    "    precision = 0.0\n",
    "    nr_relevant_items = 0\n",
    "    for index, row in ranking.iterrows():\n",
    "        if row['passage_id'] in gt:\n",
    "            nr_relevant_items += 1\n",
    "    precision = nr_relevant_items/n\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can decided for what cutoff you want to compute the precision or ndcg. For example if you want to compute P@5, set N to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check query lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the relevance label dataset contain different number of query ids. This has to do with certain decisions that were made. For example, by removing query ids for which the binary agreed assessor label was set to irrelevant while the ms marco label was originally relevant.\n",
    "\n",
    "Here we are going to check if the different label datasets actually contain different query ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_qrels_df = new_qrels_dict[\"graded\"].copy()\n",
    "binary2_qrels_df = new_qrels_dict[\"binary2\"].copy()\n",
    "binary3_qrels_df = new_qrels_dict[\"binary3\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_query_ids = get_query_ids(graded_qrels_df)\n",
    "binary2_query_ids = get_query_ids(binary2_qrels_df)\n",
    "binary3_query_ids = get_query_ids(binary3_qrels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check if all binary query ids are present in the graded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in binary2_query_ids:\n",
    "    if not query_id in graded_query_ids:\n",
    "        print(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in binary3_query_ids:\n",
    "    if not query_id in graded_query_ids:\n",
    "        print(query_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No query ids are printed so the entire binary query id list is in the graded list. This will probably also mean that both binary datasets exist of the same query ids. But lets check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in binary3_query_ids:\n",
    "    if not query_id in binary2_query_ids:\n",
    "        print(query_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again no query ids are printed so, while there are a few query ids that either belong to the binary2 and graded dataset or only to graded dataset. All other query ids are shared among the three datasets.\n",
    "\n",
    "In other words, each smaller dataset is a subset of the larger datasets.\n",
    "Exact numbers:\n",
    "\n",
    "binary3_dataset nr queries = 42\n",
    "\n",
    "binary2_dataset nr queries = 46\n",
    "\n",
    "graded_dataset nr queries = 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate BM25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
