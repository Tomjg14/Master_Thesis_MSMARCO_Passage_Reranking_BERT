{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "query_ids = list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Nr. Assessors per Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 61, 2: 18, 3: 36, 4: 1, 5: 5, 6: 2, 7: 2}\n"
     ]
    }
   ],
   "source": [
    "counter = {}\n",
    "for query_id in query_ids:\n",
    "    passage_ids = list(data[query_id].keys())\n",
    "    nr_assessors = int(len(data[query_id][passage_ids[0]][1:])/2)\n",
    "    if not nr_assessors in counter.keys():\n",
    "        counter[nr_assessors] = 1\n",
    "    else:\n",
    "        counter[nr_assessors] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes per Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(nr_assessors):\n",
    "    labels = [\"query_id\",\"passageid\",\"msmarco\"]\n",
    "    for i in range(nr_assessors):\n",
    "        labels = labels + ['user%s_id'%(i+1),'user%s_label'%(i+1)]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_data = {}\n",
    "label_data = {}\n",
    "assessor_data = {}\n",
    "for query_id in query_ids:\n",
    "    query_list = []\n",
    "    query_data = data[query_id]\n",
    "    nr_assessors = 0\n",
    "    for i,passage_id in enumerate(query_data.keys()):\n",
    "        query_list.append([query_id, passage_id] + query_data[passage_id])\n",
    "        if i == 0:\n",
    "            nr_assessors = int((len(query_data[passage_id])-1)/2)\n",
    "    dataframe_data[query_id] = query_list\n",
    "    label_data[query_id] = create_labels(nr_assessors)\n",
    "    assessor_data[query_id] = nr_assessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for query_id in query_ids:\n",
    "    df = pd.DataFrame(dataframe_data[query_id],columns=label_data[query_id])\n",
    "    dataframes[query_id] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes an assessors forgot to provide input, which means that the dataset includes \"no_input\". This is missing data and often we can fix this by considering the input of the remaining assessors. If the remaining assessors agree, I can still provide the agreed relevance label. If there is no agreement between the remaining assessors, I simply take the relevance label that MSMARCO provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What queries have missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097449\n",
      "825147\n",
      "993153\n",
      "904389\n",
      "1096257\n",
      "689885\n",
      "758519\n",
      "202306\n",
      "1077356\n",
      "nr of queries with missing data: 9\n"
     ]
    }
   ],
   "source": [
    "queries_with_missing_data = []\n",
    "counter = 0\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        if 'no_input' in df.values:\n",
    "            counter += 1\n",
    "            print(query_id)\n",
    "            queries_with_missing_data.append(query_id)\n",
    "print(\"nr of queries with missing data: %s\"%(counter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many data is missing per query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097449\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "0\n",
      "user3_label\n",
      "0\n",
      "user4_label\n",
      "1\n",
      "user5_label\n",
      "0\n",
      "user6_label\n",
      "0\n",
      "user7_label\n",
      "0\n",
      "\n",
      "\n",
      "825147\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "0\n",
      "user3_label\n",
      "1\n",
      "user4_label\n",
      "0\n",
      "user5_label\n",
      "0\n",
      "\n",
      "\n",
      "993153\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "19\n",
      "user3_label\n",
      "1\n",
      "user4_label\n",
      "0\n",
      "user5_label\n",
      "0\n",
      "user6_label\n",
      "0\n",
      "user7_label\n",
      "0\n",
      "\n",
      "\n",
      "904389\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "0\n",
      "user3_label\n",
      "1\n",
      "\n",
      "\n",
      "1096257\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "1\n",
      "user3_label\n",
      "0\n",
      "\n",
      "\n",
      "689885\n",
      "user1_label\n",
      "1\n",
      "user2_label\n",
      "0\n",
      "user3_label\n",
      "0\n",
      "\n",
      "\n",
      "758519\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "1\n",
      "user3_label\n",
      "0\n",
      "\n",
      "\n",
      "202306\n",
      "user1_label\n",
      "0\n",
      "user2_label\n",
      "1\n",
      "user3_label\n",
      "0\n",
      "\n",
      "\n",
      "1077356\n",
      "user1_label\n",
      "1\n",
      "user2_label\n",
      "0\n",
      "user3_label\n",
      "0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query_id in queries_with_missing_data:\n",
    "    print(query_id)\n",
    "    df = dataframes[query_id]\n",
    "    column_labels = list(df.columns)\n",
    "    for i in range(7):\n",
    "        column = 'user%s_label'%(i+1)\n",
    "        if column in column_labels:\n",
    "            print(column)\n",
    "            print(len(df[df[column].isin([\"no_input\"])]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most queries have just one 'no_input' entry, except for query 993153 which has 19 entries. So we need to remove this query from the data and find a way to deal with the remaining queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_with_missing_data.remove('993153')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids.remove('993153')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Missing Data\n",
    "\n",
    "As we want to change the labels into binary, we want to first check if the missing data is making it impossible to go to binary. This is the case when the other assessors did provide their input but they have no agreement. In those cases we need the missing data to make a decision on the label. The other cases where the two other assessors do agree, we do not need the missing data and we can already make a decision on the final label. So lets go find those cases first and deal with the more difficult cases afterwards.\n",
    "\n",
    "First we extract the row with the missing data and gather the assessors labels\n",
    "\n",
    "If we then change the assessors labels to binary and there is agreement, we take the agreed label as new label. If there is no agreement, we keep the msmarco label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary(label):\n",
    "    if label < 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097449\n",
      "New label: 1\n",
      "825147\n",
      "New label: 1\n",
      "904389\n",
      "Label equals MSMARCO: 0\n",
      "1096257\n",
      "New label: 0\n",
      "689885\n",
      "Label equals MSMARCO: 0\n",
      "758519\n",
      "Label equals MSMARCO: 0\n",
      "202306\n",
      "Label equals MSMARCO: 0\n",
      "1077356\n",
      "New label: 0\n"
     ]
    }
   ],
   "source": [
    "missing_value_fixes = {}\n",
    "for query_id in queries_with_missing_data:\n",
    "    df = dataframes[query_id]\n",
    "    column_labels = list(df.columns)\n",
    "    for i in range(7):\n",
    "        column = 'user%s_label'%(i+1)\n",
    "        if column in column_labels:\n",
    "            if len(df[df[column].isin([\"no_input\"])]) > 0:\n",
    "                labels = list(df[df[column].isin([\"no_input\"])].values[0][3:][1::2])\n",
    "                labels.remove(\"no_input\")\n",
    "                binary_labels = [make_binary(int(x)) for x in labels]\n",
    "                new_label = 0\n",
    "                if (len(set(binary_labels))) == 1:\n",
    "                    #there is agreement\n",
    "                    print(query_id)\n",
    "                    print(\"New label: %s\"%(binary_labels[0]))\n",
    "                    new_label = binary_labels[0]\n",
    "                else:\n",
    "                    msmarco_label = df[df[column].isin([\"no_input\"])]['msmarco'].values[0]\n",
    "                    if msmarco_label == 'irrelevant':\n",
    "                        print(query_id)\n",
    "                        print(\"Label equals MSMARCO: 0\")\n",
    "                        new_label = 0\n",
    "                    else:\n",
    "                        print(query_id)\n",
    "                        print(\"Label equals MSMARCO: 1\")\n",
    "                        new_label = 1\n",
    "                missing_value_fixes[query_id] = {}\n",
    "                missing_value_fixes[query_id][\"no_input\"] = new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1077356': {'no_input': 0},\n",
       " '1096257': {'no_input': 0},\n",
       " '1097449': {'no_input': 1},\n",
       " '202306': {'no_input': 0},\n",
       " '689885': {'no_input': 0},\n",
       " '758519': {'no_input': 0},\n",
       " '825147': {'no_input': 1},\n",
       " '904389': {'no_input': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2 cases where an assessor forget to give input, the passage will now be marked as relevant due to agreement between the remaining assessors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 OR MORE ASSESSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section helps to change the input of the assessors into binary to get either 'irrelevant' or 'relevant'. Irrelevant is set to 0 and relevant is set to 1. All input less then 3 is set to irrelevant and all input equal to 3 or higher is set to relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary: irrelevant <3; relevant >=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBinary(query_id,label):\n",
    "    if label == \"no_input\":\n",
    "        return missing_value_fixes[query_id][label]\n",
    "    elif int(label) < 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        for i in range(nr_assessors):\n",
    "            df['user%s_binary_label'%(i+1)] = df[['user%s_label'%(i+1)]].apply(lambda x: [makeBinary(query_id,y) for y in x])\n",
    "        dataframes[query_id] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        df['relevance_agreement'] = df.iloc[:, -nr_assessors:].sum(axis=1)\n",
    "        df['relevant'] = df[['relevance_agreement']].apply(lambda cell: [0 if ((((nr_assessors%2) == 0) and (value < (int(np.ceil(nr_assessors/2))+1))) or (((nr_assessors%2) == 1) and (value <(int(np.ceil(nr_assessors/2)))))) else 1 for value in cell])\n",
    "        dataframes[query_id] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find 3 sections:\n",
    "1: this section will help to compute the original assessor agreement data and save this to a .tsv file\n",
    "2: this section will help to label the missing msmarco relevant passage and save this to a .tsv file\n",
    "3: this section will help to create the actual data used for my thesis\n",
    "\n",
    "Skip sections 1 and 2 to get be able to correclte create the data used for my thesis. Otherwise run section 1 before you move to section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compute Original Assessor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agreement_data = {}\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        nr_relevant = df['relevant'].sum()\n",
    "        query_agreement_data[query_id] = nr_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_agreement_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_query_agreement_data = sorted(query_agreement_data.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('540906', 1),\n",
       " ('335710', 1),\n",
       " ('866251', 1),\n",
       " ('608323', 2),\n",
       " ('1091688', 2),\n",
       " ('427323', 2),\n",
       " ('1034595', 2),\n",
       " ('208494', 2),\n",
       " ('321951', 3),\n",
       " ('1056446', 4),\n",
       " ('202306', 4),\n",
       " ('904389', 4),\n",
       " ('75266', 4),\n",
       " ('54819', 4),\n",
       " ('30860', 4),\n",
       " ('1049791', 4),\n",
       " ('149161', 5),\n",
       " ('212195', 5),\n",
       " ('831784', 5),\n",
       " ('178468', 5),\n",
       " ('414155', 6),\n",
       " ('785721', 6),\n",
       " ('1040461', 7),\n",
       " ('1096257', 7),\n",
       " ('838453', 7),\n",
       " ('341317', 8),\n",
       " ('1003875', 8),\n",
       " ('689885', 8),\n",
       " ('825147', 9),\n",
       " ('1084469', 12),\n",
       " ('741392', 13),\n",
       " ('1097449', 13),\n",
       " ('1077356', 13),\n",
       " ('758519', 13),\n",
       " ('208822', 14),\n",
       " ('409143', 14),\n",
       " ('687375', 15),\n",
       " ('1006199', 16),\n",
       " ('242107', 16),\n",
       " ('440362', 16),\n",
       " ('993987', 16),\n",
       " ('1083663', 16),\n",
       " ('1007473', 18),\n",
       " ('117113', 19),\n",
       " ('1086248', 19)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_query_agreement_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the agreement data it can be noted that a few queries just have 1 relevant passage according to the assessors. After inspecting those queries, it is surprising to see that often these relevant passages are not the same passages appointed by MSMARCO. It is clear that the assessors do not always notice the msmarco relevant passage. This means that in order for us to work with this data we should go over all queries and check if the msmarco relevant passage is actually marked as relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to work with the data, I will go and check which queries have passages marked as irrelevant by assessors but are relevant according to msmarco. Those queries will be removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save New Labels to txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to save the work of our assessors for now. So I can always decide how to work with the data. Later on I will add missing msmarco relevance labels and decide which queries we leave out of the dataset because of not enough relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_queries = []\n",
    "relevant_passages = []\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        relevance_df = df[df['relevant'] == 1]\n",
    "        relevant_queries = relevant_queries + relevance_df['query_id'].values.tolist()\n",
    "        relevant_passages = relevant_passages + relevance_df['passageid'].values.tolist()\n",
    "output_df = pd.DataFrame()\n",
    "output_df['query_id'] = relevant_queries\n",
    "output_df['label1'] = 0\n",
    "output_df['passage_id'] = relevant_passages\n",
    "output_df['label2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('original_assessors_relevance_labels.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Add MSMARCO Relevance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MSMARCO Relevant Passages (if not already included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below helps to find those msmarco relevant passages and set them to relevant if the assessors did not label them as relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tomjg\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "final_dataframes = {}\n",
    "counter = 0\n",
    "list_ids = []\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        final_df = df.copy()\n",
    "        idx = final_df.index[(final_df['msmarco'] == 'relevant') & (final_df['relevant'] == 0)]\n",
    "        if not (idx.values.size == 0):\n",
    "            counter += 1\n",
    "            list_ids.append(query_id)\n",
    "            final_df.set_value(idx[0],'relevant',1)\n",
    "        final_dataframes[query_id] = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_query_agreement_data = {}\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = final_dataframes[query_id]\n",
    "        nr_relevant = df['relevant'].sum()\n",
    "        final_query_agreement_data[query_id] = nr_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_agreement_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_final_query_agreement_data = sorted(final_query_agreement_data.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('335710', 1),\n",
       " ('608323', 2),\n",
       " ('1091688', 2),\n",
       " ('1034595', 2),\n",
       " ('540906', 2),\n",
       " ('866251', 2),\n",
       " ('208494', 2),\n",
       " ('427323', 3),\n",
       " ('1056446', 4),\n",
       " ('202306', 4),\n",
       " ('321951', 4),\n",
       " ('904389', 4),\n",
       " ('75266', 4),\n",
       " ('54819', 4),\n",
       " ('30860', 4),\n",
       " ('1049791', 4),\n",
       " ('149161', 5),\n",
       " ('212195', 5),\n",
       " ('831784', 5),\n",
       " ('178468', 5),\n",
       " ('414155', 6),\n",
       " ('785721', 6),\n",
       " ('1040461', 7),\n",
       " ('1096257', 7),\n",
       " ('341317', 8),\n",
       " ('838453', 8),\n",
       " ('1003875', 8),\n",
       " ('689885', 8),\n",
       " ('825147', 9),\n",
       " ('1084469', 12),\n",
       " ('741392', 13),\n",
       " ('1097449', 13),\n",
       " ('1077356', 13),\n",
       " ('758519', 13),\n",
       " ('208822', 14),\n",
       " ('409143', 14),\n",
       " ('687375', 15),\n",
       " ('1006199', 16),\n",
       " ('242107', 16),\n",
       " ('440362', 16),\n",
       " ('1083663', 16),\n",
       " ('993987', 17),\n",
       " ('1007473', 18),\n",
       " ('117113', 19),\n",
       " ('1086248', 19)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_final_query_agreement_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': [<matplotlib.lines.Line2D at 0x226ff1135f8>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x226ff113da0>,\n",
       "  <matplotlib.lines.Line2D at 0x226ff126128>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x226ff1267b8>],\n",
       " 'means': [],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x226ff126470>],\n",
       " 'whiskers': [<matplotlib.lines.Line2D at 0x226ff09f940>,\n",
       "  <matplotlib.lines.Line2D at 0x226ff113a58>]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(list(final_query_agreement_data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Complete Data to tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the msmarco relevant passages to the dataframes, query '335710' still only has 1 relevant passage. So we will need to remove this query from the dataset. The remaining queries have more than 1 relevant passage and so are of interest for our study. This means we will have 44 queries to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids.remove('335710')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_queries = []\n",
    "relevant_passages = []\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = final_dataframes[query_id]\n",
    "        relevance_df = df[df['relevant'] == 1]\n",
    "        relevant_queries = relevant_queries + relevance_df['query_id'].values.tolist()\n",
    "        relevant_passages = relevant_passages + relevance_df['passageid'].values.tolist()\n",
    "output_df = pd.DataFrame()\n",
    "output_df['query_id'] = relevant_queries\n",
    "output_df['label1'] = 0\n",
    "output_df['passage_id'] = relevant_passages\n",
    "output_df['label2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('complete_relevance_labels.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Actual Relevance Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will help to create the dataset that is used for my thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 queries for which the assessors did not label the MSMARCO relevant passage as relevant. I do not want to manually label this passage as relevant, because by doing so I am actually denying the assessments by the assessors. This is why I will remove these queries from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather ids to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First gather all queries where the msmarco relevant passage is not recognized as relevant by the assessors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_2_remove = []\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        idx = df.index[(df['msmarco'] == 'relevant') & (df['relevant'] == 0)]\n",
    "        if not (idx.values.size == 0):\n",
    "            ids_2_remove.append(query_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then gather all queries that (when binarized and after majorith voting) have only the msmarco passage as relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agreement_data = {}\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        df = dataframes[query_id]\n",
    "        nr_relevant = df['relevant'].sum()\n",
    "        query_agreement_data[query_id] = nr_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id, nr_relevant_passages in query_agreement_data.items():\n",
    "    if nr_relevant_passages < 2:\n",
    "        if not query_id in ids_2_remove:\n",
    "            ids_2_remove.append(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['866251', '427323', '838453', '540906', '993987', '321951', '335710']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_2_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in ids_2_remove:\n",
    "    query_ids.remove(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_queries = []\n",
    "relevant_passages = []\n",
    "counter = 0\n",
    "for query_id in query_ids:\n",
    "    nr_assessors = assessor_data[query_id]\n",
    "    if nr_assessors >= 3:\n",
    "        counter += 1\n",
    "        df = dataframes[query_id]\n",
    "        relevance_df = df[df['relevant'] == 1]\n",
    "        relevant_queries = relevant_queries + relevance_df['query_id'].values.tolist()\n",
    "        relevant_passages = relevant_passages + relevance_df['passageid'].values.tolist()\n",
    "output_df = pd.DataFrame()\n",
    "output_df['query_id'] = relevant_queries\n",
    "output_df['label1'] = 0\n",
    "output_df['passage_id'] = relevant_passages\n",
    "output_df['label2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('Multiple_Relevance_Dataset.tsv',sep='\\t',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
